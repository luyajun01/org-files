#+TITLE: 统计理论wiki
# #+SETUPFILE: /Users/luyajun/org-html-themes/setup/theme-bigblow.setup
#+OPTIONS: TeX:t LaTeX:t TOC:nil
#+latex_class: article
#+latex_compiler: xelatex
#+OPTIONS: author:nil email:nil creator:nil timestamp:nil html-postamble:nil TOC:t
# #+HTML_HEAD: <link href="/Users/luyajun/Documents/坚果云/我的坚果云/github/code/css/org-mode.css" rel="stylesheet" type="text/css">
#+STARTUP: inlineimages

 #+BEGIN_SRC quote
如果你对数据绝对没有任何假设，那么你没有理由会更偏好于某个模型。  —— “没有免费午餐定理”
有时，发现问题，比解决问题更困难。
 #+END_SRC
* 问题

* 概率论
** 收敛理论
依概率收敛：
\begin{equation}
 \lim _{n \rightarrow \infty} p\left(\left|x_{n}-x\right| \geqslant \varepsilon\right)=0 \quad x_{n} \stackrel{p}{\rightarrow} x 
\end{equation}
以概率 1 收敛：
\begin{equation}
 P\left(\lim_{n \rightarrow \infty} X_{n}=X\right)=1 \quad x_{n} \stackrel{\text { a.s. }}{\rightarrow} X 
\end{equation}

依分布收敛：
\begin{equation}
 \lim _{n \rightarrow \infty} p\left(x_{n} \leqslant x\right)=P(X \leqslant x) \quad X_{n} \stackrel{d}{\rightarrow} x 
\end{equation}

可以证明依概率收敛可以推出依分布收敛。

* 有监督模型 

知名的有监督模型包括：K-近邻算法、线性回归、逻辑回归、支持向量机、决策树和随机森林、神经网络等。

** 回归模型理论

根据自变量因子的性质，可以将线性模型分为三类：

1、凡自变量因子都是数量因子， **就称为这个模型是回归分析模型**;

2、如果自变量因子均为属性变量， **则称为模型是方差分析模型**;

3、倘若自变量因子中，既有属性因子，也有数量因子, **就称为协方差分析模型**.
** 一般线性回归模型

假定因变量 $Y$ 和自变量 $X$ 满足线性回归模型，其方程为：

\[
Y=X\beta+\epsilon
\]

式中，因变量 $Y$ 为 $n$ 维向量；自变量 $X$ 为 $n\times p$ 矩阵；误差项 $\epsilon$ 为 $n$ 维向量。需要注意的是在简单回归中，误差项 $\epsilon$ 的元素一般要求是独立同分布零均值的，而通常分布假定为正态的，在最小二乘回归的标准输出中，对系数的 $t$ 检验和方差分析的 F 检验，常常认为 p 值小就意味着“显著”，但需要注意误差是否偏离正态性，如果不考虑正态性或者渐近正态性不成立，那么 t 检验和 F 检验就没有任何意义。

在模型比较过程中，需要注意的是对于不满足正态性假定的模型也可以进行互相比较，但所用方法不是这些基于正态性的检验，可以用 AIC 之类的准则或交叉验证来比较。

#+begin_src python
import numpy as np
from sklearn.linear_model import LinearRegression
X=np.array([[1,1],[1,2],[2,2],[2,3]])
y = np.dot(X, np.array([1, 2])) + 3
reg = LinearRegression().fit(X, y)
reg.score(X, y)
reg.coef_
reg.intercept_
reg.predict(np.array([[3, 5]]))
#+end_src


#+begin_src python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
from sklearn.linear_model import LinearRegression
from sklearn.isotonic import IsotonicRegression
from sklearn.utils import check_random_state
n = 100
x = np.arange(n)
rs = check_random_state(0)
y = rs.randint(-50, 50, size=(n, )) + 50.*np.log1p(np.arange(n))

################fit isotonic-regression
ir = IsotonicRegression()
y_ = ir.fit_transform(x, y)
lr = LinearRegression()
lr.fit(x[:, np.newaxis], y)

segments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]
lc = LineCollection(segments, zorder=0)
lc.set_array(np.ones(len(y)))
lc.set_linewidths(np.full(n, 0.5))

fig = plt.figure()
plt.plot(x, y, "r.", markersize=12)




  
#+end_src


#+BEGIN_SRC Python
import numpy as np
import scipy as sp
from scipy.optimize import leastsq
import matplotlib.pyplot as plt
# 目标函数
def real_func(x):
    return np.sin(2*np.pi*x)

# 多项式
def fit_func(p, x):
    f=np.polyy1d(p)
    return f(x)

# 残差
def residuals_func(p, x, y):
    ret=fit_func(p, x) - y
    return ret

# 构造10个点
x=np.linspace(0, 1, 10)
x_points=np.linspace(0, 1, 1000)

#+END_SRC 

#+BEGIN_SRC Python
from sklearn import line
from sklearn import linear_model

#+END_SRC

** TODO 回归诊断
** 方差分析（ANOVA）
** logistic 回归模型
*** 模型形式
Sppose the response variable $Y_{i}$ for $i=1,\cdots,n_{i}$ is binomially distributed $B(n_{i},p_{i})$ so that:
\begin{equation}
P\left(Y_{i}=y_{i}\right)=\left(\begin{array}{l}{n_{i}} \\ {y_{i}}\end{array}\right) p_{i}^{y_{i}}\left(1-p_{i}\right)^{n_{i}-y_{i}}
\end{equation}

we further assume that the $Y_{i}$ are independent.The individual trials that compose the response $Y_{i}$ are all subject to the same $q$ predictors $(x_{i1},\cdot,x_{iq})$.The group of trials is known as a /covariate class/. we need a model that describes the relationship of $x_{1},\cdot,x_{q}$ to $p$.Following the linear model approach, we construct a /linear predictor/:

\begin{equation}
\eta_{i}=\beta_{0}+\beta_{1} x_{i 1}+\ldots+\beta_{q} x_{i q}
\end{equation}

we have already seen above that setting $\eta_{i}=p_{i}$ is not appropriate because we require $0 \leq p_{i} \leq 1$.Instead we shall use a link function $g$ such that $\eta_{i}=g(p_{i})$.For this application,we shall need $g$ to be monotone and be such that $0\leq g^{-1}(\eta)\leq 1$ for any $\eta$.There are three common choices: 

1.logit:$\eta=log(p/(1-p))$.
2.probit:$\eta=\Phi^{-1}(p)$ where $\Phi^{-1}$ is the inverse normal cumulative distribution function.
3.Complementary log-log:$\eta=\log(-log(1-p))$


\[
\operatorname{logit}\left(p_{i}\right)=\log \left(\frac{p_{i}}{1-p_{i}}\right)=\beta_{0}+\beta_{1} x_{1 i}+\cdots+\beta_{k} x_{k i}
\]

$$
p_{i}=\frac{\exp \left(\beta_{0}+\beta_{1} x_{1 i}+\cdots+\beta_{k} x_{k i}\right)}{1+\exp \left(\beta_{0}+\beta_{1} x_{1 i}+\cdots+\beta_{k} x_{k i}\right)}
$$

观测 $y_{i}$ 服从于一个二项分布，均值是 $n_{i}p_{i}$,能够表示为 $y_{i}=n_{i}p_{i}+\epsilon_{i}$,残差部分 $\epsilon_{i}=y_{i}-n_{i}p_{i}$ 是零均值，但是不再服从的是二项分布，实际上， $\epsilon$ **服从的是位移二项分布**.

需要补充的是：$E\left(\varepsilon_{i} | X_{i}\right)=0$,即给定 X 的前提下，$\varepsilon_{i}$ 的期望为 0. 

\begin{equation}
 \varepsilon_{j}=\left\{\begin{array}{ll}{1-X_{j}^{\prime} \beta} & {\left(Y_{i}=1\right)} \\ {-X_{i}^{\prime} \beta} & {\left(Y_{j}=0\right)}\end{array}\right. 
\end{equation}

上式为 logistic 回归模型的残差，可以看出是二元变量，而不是我们通常假定的正态分布。

【问题？】
什么是位移二项分布？

似然函数

$$
L(\boldsymbol{\beta})=\prod_{i=1}^{n}\left(\begin{array}{l}{n_{i}} \\ {y_{i}}\end{array}\right) p_{i}^{y_{i}}\left(1-p_{i}\right)^{n_{i}-y_{i}}
$$

\begin{aligned} \log L(\boldsymbol{\beta}) &=\sum_{i=1}^{n}\left\{\log \left(\begin{array}{l}{n_{i}} \\ {y_{i}}\end{array}\right)+y_{i} \log p_{i}+\left(n_{i}-y_{i}\right) \log \left(1-p_{i}\right)\right\} \\ &=\sum_{i=1}^{n}\left\{\log \left(\begin{array}{c}{n_{i}} \\ {y_{i}}\end{array}\right)+y_{i} \log \left(\frac{p_{i}}{1-p_{i}}\right)+n_{i} \log \left(1-p_{i}\right)\right\} \\ &=\sum_{i=1}^{n}\left\{\log \left(\begin{array}{l}{n_{i}} \\ {y_{i}}\end{array}\right)+y_{i} \eta_{i}-n_{i} \log \left(1+e^{\eta_{i}}\right)\right\} \end{aligned}


$$
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_{j}}=\sum_{i=1}^{n} y_{i} x_{j i}-\sum_{i=1}^{n} n_{i} x_{j i} e^{\eta_{i}}\left(1+e^{\eta_{i}}\right)^{-1}, \quad j=0,1, \ldots, k
$$

可知，上式是没有办法求解 $\beta$ 的精确解，只能求得数值解。一个广泛的求解方法就是 Fisher ’s 得分法，此法相当于是一个重复加权最小二乘方法, $z_{i}=\eta_{i}+(y_{i}-n_{i}p_{i})/\{n_{i}p_{i}(1-p_{i})\}$,weight 等于 $n_{i}p_{i}(1-p_{i})$.

一些有用的结论：

- 随着样本量 $n$ 的增加，二项分布近似于正态分布.随机变量 $Z=\frac{Y-n p}{\sqrt{\{} n p(1-p)\}}$ 接近正态分布。McCullagh 等人证明当 $np(1-p)\ge2$,随机变量 Y 即可满足正态分布假设，特别是当 $p$ 接近于 0.5 的时候，所以当 n 等于 10 的时候，二项分布近似于正态分布。

在样本量足够大时，二项分布近似于正态分布。其实也可以利用线性模型拟合逾期率：

$y_{i}=0$ 为失败，$y_{i}=1$ 为成功。$E(Y_{i})=n_{i}p_{i}$,$var(Y_{i})=n_{i}p_{i}(1-p_{i})$

$$
\sum_{i=1}^{n}\left(\frac{y_{i}}{n_{i}}-p_{i}\right)^{2}=\sum_{i=1}^{n}\left(\tilde{p}_{i}-\beta_{0}-\beta_{1} x_{1 i}-\cdots-\beta_{k} x_{k i}\right)^{2}
$$

但是这种方法有很多的缺点：比如，异方差问题，$\operatorname{var}\left(\tilde{p}_{i}\right)=p_{i}(1-p_{i})/n_{i}$,当 $p_{i}$ 在 0.25-0.75 时, \(0.19 < p_{i}(1-p_{i}) < 0.25\) 也就是方差不会相差很大，但是 p 值很大或者很小的时候，那么方差变化就会很大！一种解决的方法就是加权回归模型 $\sum_{i=1}^{n} w_{i}\left(\tilde{p}_{i}-p_{i}\right)^{2}$.第二个问题就是正态分布，当 n 很大的时候，这个问题不存在。第三个问题就是，估计值可能是负数！而 $\hat{p}$ 不可能是负数！

所以，需要对成功概率 $p$ 作 logit 变换 $log(p/(1-p))$,可以写作 $logit(p)$.logit 变换后，值域就变成了 $(-\inf,\inf)$.

simulation of glm

#+BEGIN_SRC R :exports both :results graphics :file ./fig_1.png
  ##两个特征高度相关
  library(MASS)
  n=1000
  #inv.logit 其实就是 P
  inv.logit <- function(p){
      return(exp(p)/(1+exp(p)))
  }
  Sigma <- matrix(c(1,0.9,0.9,1),2,2)
  X=mvrnorm(n = 1000, rep(0, 2), Sigma)
  beta1=c(0.5,1.5)
  Y=rbinom(n,1,inv.logit(1+X%*%beta1+rnorm(1000,0,1)))
  data=data.frame(Y,X)
  glm(Y~1+.,data = data,family = "binomial")
  #####特征重复2份
  library(MASS)
  x1=rnorm(1000,mean = 0,sd=1)
  X=matrix(rep(x1,2),nrow = 1000)
  beta1=c(0.5,1.5)
  Y=rbinom(n,1,inv.logit(1+X%*%beta1+rnorm(1000,0,1)))
  data=data.frame(Y,X)
  glm(Y~1+.,data = data,family = "binomial")
#+END_SRC

*** 模型推断

假设有

*** 变量编码
**** woe 编码

证据权重的优点是特征变量的数量不会增加（虚拟变量要生成其他变量），所以不同变量之间相关的可能性会变得更小，且在统计估计时稳健性也会更好。
但缺点是只可以选择性地保留某个特征的全部属性或者一个也不保留。使用虚拟变量时，由于每个特征会生成多个变量，而很正常的是，某个评分卡只用到其中的一些属性变量，但这些属性却被其他评分卡剔除。


#+BEGIN_SRC R :exports both :results graphics :file ./fig_1.png
mifi_model_feature_woe_encoding_all(df, feat_cuts, category_feature_names = NULL,
  label_identify, encoding_path, missing_val = -1, is_debug = F)
#+END_SRC

**** one-hot 编码

**** dummy 编码

*** 模型评估

混淆矩阵：

|      |                       预测              |
| 实际 | 1                      | 0                |
|------+------------------------+------------------|
|    1 | d，true positive       | c,false positive |
|    0 | b,false negative       | a,true negative  |
|      | b+d,predicted positive | a+c,predicted negative |


- Sensitivity（覆盖率，True Positive Rate）= 正确预测到的正例数 / 实际正例总数 Recall (True Positive Rate，or Sensitivity) =true positive/total actual positive=d/c+d
    
- PV+ (命中率，Precision, Positive Predicted Value) = 正确预测到的正例数 / 预测正例总数 Precision (Positive Predicted Value, PV+) =true positive/ total predicted positive=d/b+d

- Specificity (负例的覆盖率，True Negative Rate) = 正确预测到的负例个数 / 实际负例总数 Specificity (True Negative Rate) =true negative/total actual negative=a/a+b

Ptp=proportion of true positives=d/a+b+c+d=(c+d/a+b+c+d)*(d/c+d) =pi1* Sensitivity，正确预测到的正例个数占总观测值的比例

Pfp=proportion of false positives=b/a+b+c+d= (a+b/a+b+c+d)*(b/a+b) = (1-c+d/a+b+c+d)*(1-a/a+b) = (1-pi1)*(1- Specificity) ，把负例错误地预测成正例的个数占总数的比例

Depth=proportion allocated to class 1=b+d/a+b+c+d=Ptp+Pfp，预测成正例的比例

PV_plus=Precision (Positive Predicted Value, PV+) = d/b+d=Ptp/depth，正确预测到的正例数占预测正例总数的比例

Lift= (d/b+d)/(c+d/a+b+c+d)=PV_plus/pi1，提升值

*** 回归诊断     

总体来说，判断一个变量是否应该进入评分卡的其中一种方法是有无该变量的两个模型的数据拟合度变化情况。

Suppose that a linear logistic model is ﬁtted to n binomial observations of the form $y_{i}/n_{i},i=1,2,\cdots,n$,对应的拟合值 $y_{i}$ 就是 $\hat{y}_{i}=n_{i}\hat{p}_{i}$.The ith raw residual is then the difference $y_{i}-\hat{y}_{i}$, and provides information about how well the model ﬁts each particular observation.

- *标准 Pearson 误差*
            
The raw residuals can be made more comparable by dividing them by $se(y_{i})$, giving

\[
X_{i}=\frac{y_{i}-n_{i} \hat{p}_{i}}{\left.\sqrt{\{} n_{i} \hat{p}_{i}\left(1-\hat{p}_{i}\right)\right\}}
\]

这个残差常被称为“Pearson residuals”,因为它们的平方和统计量 $X^{2}=\sum X_{i}^{2}$,被称为 Pearson's 卡方统计量.

更优的统计量是 A better procedure is to divide the raw residuals by their standard error，\(\operatorname{se}\left(y_{i}-\hat{y}_{i}\right)\),

\[
\left.\operatorname{se}\left(y_{i}-\hat{y}_{i}\right)=\sqrt{\{} \hat{v}_{i}\left(1-h_{i}\right)\right\}
\]

\(\hat{v}_{i}=n_{i} \hat{p}_{i}\left(1-\hat{p}_{i}\right)\), $h_{i}$ is the $ith$ diagonal element of the $n\times n$ matrix \(\boldsymbol{H}=\boldsymbol{W}^{1 / 2} \boldsymbol{X}\left(\boldsymbol{X}^{\prime} \boldsymbol{W} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{W}^{1 / 2}\).

这样很容易得出标准的残差：

$$
r_{P i}=\frac{y_{i}-n_{i} \hat{p}_{i}}{\left.\sqrt{\{} \hat{v}_{i}\left(1-h_{i}\right)\right\}}
$$

- *标准 deviance 误差*

deviance D measures how close the (smaller) model comes to perfection, is a measure of goodness of fit.


Another type of residual can be constructed from the deviance that is obtained after ﬁtting a linear logistic model to binomial data, given by

\begin{equation}
D=2 \sum_{i}\left\{y_{i} \log \left(\frac{y_{i}}{\hat{y}_{i}}\right)+\left(n_{i}-y_{i}\right) \log \left(\frac{n_{i}-y_{i}}{n_{i}-\hat{y}_{i}}\right)\right\}
\end{equation}

The signed square root of the contribution of the ith observation to this overall deviance is

\begin{equation}
d_{i}=\operatorname{sgn}\left(y_{i}-\hat{y}_{i}\right)\left\{2 y_{i} \log \left(\frac{y_{i}}{\hat{y}_{i}}\right)+2\left(n_{i}-y_{i}\right) \log \left(\frac{n_{i}-y_{i}}{n_{i}-\hat{y}_{i}}\right)\right\}^{1 / 2}
\end{equation}

$d_{i}$ 可以成为 deviance 误差，那么总体的误差可以称为 $D=\sum d^{2}_{i}$,那么标准 deviance 误差为

\begin{equation}
r_{D i}=\frac{d_{i}}{\sqrt{\left(1-h_{i}\right)}}
\end{equation}

- conclusion

The numerical studies also indicate that all three of these residuals are reasonably well approximated by a standard normal distribution when the binomial denominators are not too small.  (为啥当 N 趋近于无穷时，二项分布逼近于正态分布？)

deviance 也被称作偏差，计算公式为“－2*ln（当前模型的似然值/饱和模型的似然值）”，这个统计量服从 $\chi^2$ 分布，服从自由度 1.
#+BEGIN_SRC R :results graphics :file fig_1.png :exports both
#样本量小的时候，可以看到二项分布和正态分布有很大差异，而样本量大，确实很相近
  n=10
  p=0.1
  q=1-p
  x=0:10
  y=dbinom(x,n,p)
  plot(x,y,type="h",lwd=2,col="red")
  n=100
  p=0.1
  q=1-p
  x=0:100
  y=dbinom(x,n,p)
  plot(x,y,type="h",lwd=2,col="red")
#+END_SRC

*** 统计检验
**** wald 检验

If the hypothesis involves only a single parameter restriction, then the Wald statistic takes the following form: 

\(W=\frac{\left(\hat{\theta}-\theta_{0}\right)^{2}}{\operatorname{var}(\hat{\theta})}\)

which under the null hypothesis follows an asymptotic $\chi^2$-distribution with one degree of freedom. 

https://en.wikipedia.org/wiki/Wald_test

**** 卡方检验

这是一种评估数据拟合特定统计模型程度的普遍方法，计算真实和预测结果误差的平方和，然后以方差来标准化。
举例来说，卡方统计量就是好人、坏人预测数量（label=0）与观测数量之差的平方和，再除以理论方差。

*** 实际应用
**** 评分卡

至少有三种方式可以去评估一个评分系统的有效性：

1. 评分卡的判别能力。测量评分卡区分好人与坏人的能力。

2. 评分卡概率预测的校准精度。它要求将评分分数转换为事件的发生概率的函数。

3. 评分卡分类的划分的准确程度

**** lift chart

Lift（提升指数）是评估一个预测模型是否有效的一个度量；这个比值由运用和不运用这个模型所得来的结果计算而来。提升指数越大，模型的运行效果越好。

建立步骤：

I) 利用已经建立的评分模型，对我们要验证的样本进行评分。样本下的每一个个体都将得到一个分数，或者是违约概率，或者是一个分值；

II) 对样本按照上面计算好的分数进行降序排序；

III) 把已经排好序的样本依次分成 10 个数量相同的群体，我们就建立了一个叫 decile 的变量，它依次取 10 个值，1、2、3、4、5、6、7、8、9、10，diclie1 包括违约概率值最高的 10%的个体，diclie2 包括下一个 10%的群体，以此类推；

IV) 帐户总数是每个 decile 下的样本数，它是整个样本数的 10%；

V) 边际坏账数是每个 decile 内违约的人数，就是说，利用我们的评分模型，在 decile1，有 25 个人违约，以此类推，从定义来看这个边际坏账数应该是单调递降的；

VI) 累计坏账数，45 表明前两个 decile 内共有 45 个人违约，以此类推；

VII) 边际坏账率是每个 decile 内坏账的比率。对 decile1，边际坏账率由 25/100 得来；

VIII) 对每一个加总的 decile，都计算一个累计坏账率，比如说，对前两个 decile，也就是整个样本的 20%，累计坏账率等于（25+20）/（100+100）；

IX) 在每个 decile 里，提升指数（Lift）就是 *相应的累计坏账率与平均坏账率的偏离程度，* 计算公式是（累计坏账率-平均坏账率）/平均坏账率，习惯上还会乘上一个 100。

X) 注：在一些处理中，提升指数直接由每个 decile 的累计坏账率除以平均坏账率得来，它们之间就相差 1，一个是相对偏离，一个是绝对偏离。

XI) 就我们考察的信用评分模型，它的目的就是尽可能把人群区别来开来，比如说“好”的顾客、 “坏”的顾客。提升指数越大，表明模型运作效果越好。

理想的提升图应该在很高的提升值上保持一段，或缓慢下降一段，然后迅速下降到 1。

#+BEGIN_SRC R :exports both :results graphics :file ./fig_1.png
  require(ROCR)
  data(ROCR.simple)
  data <- as.data.frame(ROCR.simple)[1:10, ]
  data <- data[order(data[, 1], decreasing = TRUE), ]
data$rpp <- row(data[, 1, drop = FALSE])/nrow(data)
data$target_cum <- cumsum(data[, "labels"])
data$tpr <- data$target_cum/sum(data[, "labels"])
data$lift <- data$tpr/data$rpp
data
#+END_SRC

Lift = (d/b+d)/(c+d/a+b+c+d).它衡量的是，与不利用模型相比，模型的预测能力 “变好” 了多少。不利用模型，我们只能利用 “正例的比例是 c+d/a+b+c+d” 这个样本信息来估计正例的比例（baseline model），而利用模型之后，我们不需要从整个样本中来挑选正例，只需要从我们预测为正例的那个样本的子集（b+d）中挑选正例，这时预测的准确率为 d/b+d。

**** gains

Gains (增益) 与 Lift （提升）相当类似：Lift chart 是不同阈值下 Lift 和 Depth 的轨迹，Gains chart 是不同阈值下 PV + 和 Depth 的轨迹，而 PV+=lift * pi1= d/b+d（见上），所以它们显而易见的区别就在于纵轴刻度的不同.

**** k-s 统计量

KS 的计算步骤如下：

- 计算每个评分区间的好坏账户数（计算的是特征的 KS 的话，是每个特征对应的好坏账户数）。
 
- 计算每个评分区间的累计好账户数占总好账户数比率(good%)和累计坏账户数占总坏账户数比率(bad%)。 

- 计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值（累计 good%-累计 bad%），然后对这些绝对值取最大值即得此评分卡的 KS 值。



*** 广义线性混合模型（glmm）
成组的数据通常存在在任何的领域，特别对于纵向或者空间数据。对于分析成组的数据的时候，由于组内数据具有更为相似的相关性，所以再假定样本间的独立性便不再合适，所以需要引入随机效应的概念。

给定一个随机效应 $\alpha$,观测值 $y_{1},\cdots,y_{n}$ 是条件独立的，以至于 $y_{i} \sim N(x^{'}_{i}\beta+z^{'}_{i}\alpha,\tau^2)$,$x_{i},z_{i}$ 是已知的向量，$\tau^2$ 是未知的方差参数。

glmm 中，两个重要的特性是条件独立性，给定随机效应时，服从一个条件分布；第二个是随机效应的分布。
#+BEGIN_QUOTE
In mixed-effects models at least one of the covariates is a categorical covariate representing experimental or observational “units” in the data set.cite:Bates
#+END_QUOTE

这说明在混合模型中，必须要有一个分类变量，如果特征是一个随机样本，那么这个模型就是随机效应模型。固定效应和随机效应的区别在于是否是分类变量。随机效应不是一个参数而是一个未观测随机变量。

**** Definitions 

A mixed model incorporates two random variables:\(\mathscr{B}\),the $q$ -dimensional vector of random effects, and \(\mathscr{Y}\),the $n$ -dimensional response vector.In a linear mixed model the unconditional distribution of $\mathscr{B}$ and the conditions distribution,\((\mathscr{Y} | \mathscr{B}=\mathbf{b})\), are both mulvariate Gaussian (or "normal") distribution,

\begin{aligned}
(\mathscr{Y} | \mathscr{B}=&\mathbf{b}) \sim \mathscr{N}\left(\mathbf{X} \beta+\mathbf{Z} \mathbf{b}, \sigma^{2} \mathbf{I}\right) \\ \mathscr{B} & \sim \mathscr{N}\left(\mathbf{0}, \Sigma_{\theta}\right) 
\end{aligned}

The conditional mean of $\mathscr{Y}$,given $\mathscr{B}=\mathbf{b}$, is the linear predictor, $\mathbf{X} \beta+\mathbf{Z} \mathbf{b}$,which depends on the $p$-dimensional fixed-effects parameter, $\beta$ ,and on $\mathbf{b}$.The model matrices, $\mathbf{X}$ and $\mathbf{Z}$,of dimension $n\times p$ and $n\times q$,respectively,are determined from the formula for the model and the values of covariates. *Although the matrix $\mathbf{Z}$ can be large (i.e. both $n$ and $q$ can be large), it is sparse (i.e. most of the elements in the matrix are zero).*

** TODO 待写

the relative covariance factor, $

#+BEGIN_SRC R :results output graphics :file fig_2.png :exports both
    library(lme4)
    library(ggplot2)
    library(tidyverse)
    str(Dyestuff)
    head(Dyestuff)
    summary(Dyestuff)
    ## Batch     Yield     
    ## A:5   Min.   :1440  
    ## B:5   1st Qu.:1469  
    ## C:5   Median :1530  
    ## D:5   Mean   :1528  
    ## E:5   3rd Qu.:1575  
    ## F:5   Max.   :1635
    ###从Batch 的数据结构可以看出是平衡的
    p <- ggplot(Dyestuff, aes(x=Batch, y=Yield)) + geom_boxplot()
    p #可以看出不同组数据的均值不同
  #+END_SRC

  #+RESULTS:
  [[file:fig_2.png]]

  An example of linear mixed model in r 

  #+BEGIN_SRC R :exports both :results graphics :file ./fig_1.png
  library(lme4)
  fm01 <- lmer(Yield~ 1|Batch,Dyestuff,REML = FALSE,verbose = 1)
  summary(fm01)
  fm01@pp
  #+END_SRC

*** 模型校准
- 缘由
一个分类模型能否用于预测垃圾邮件、一个分子的毒性状态， *我们期望估计的类概率更能代表样本真实潜在的概率。* (在小米小贷的应用场景中，由于样本自身的偏倚性，意思就是没有拒绝样本的表现，所以样本真实的拒绝概率势必需要校准。)为了达到良好校准，概率必须有效反映出感兴趣事件的真实似然。以垃圾邮件过滤为例，如果模型预测一封邮件为垃圾邮件的概率（或概率类似值）为 20%，若平均每 5 个样本中真正有 1 个类似类型的邮件， *那么这个概率是良好校准的* 。

 *一种评估类概率的方法* 是 *校准图* （calibration plot），对于给定的数据集，校准图展示 *事件的观测概率与预测的类概率之间的对比度量* 。创建校准图的一种方法是用 分类模型对收集的已知结果（测试集效果更佳）样本进行打分，接下来基于样本的类概率对数据封箱成不同的组。例如，封箱集合可能是[0,10%],(10%,20%],...,(90%,100%].

对于每一箱，确定观测事件发生率，假定有 50 个样本落入[0,10%]箱子且仅有一个事件发生，则箱的中点是 5%，观测事件的发生率为 2%。校准图以箱的中点为 x 轴，观测事件发生率为 y 轴绘图。如果点落在沿 45%的对角线上，模型就具备良好的校准率。

 *总结：这个校准图方法可以作为不同模型估计效果的比较来看。*

如果发现模型低估了事件的概率，那么怎么办？我们可以创建另外一个模型对该模式进行调整。我们可以通过以下公式，基于训练集的类预测和真实结果对估计的概率进行后续处理：

\begin{equation}
\hat{p}^{*}=\frac{1}{1+\exp \left(-\beta_{0}-\beta_{1} \hat{p}\right)}
\end{equation}

其中，通过非校准类概率（$\hat{p}$）的函数预测真实类来估计参数。

instead of predicting the label, many applications require a posterior class probability \(P(y=1|x)\).Platt(2000) proposes approximating the posterior by a sigmoid Function
\begin{equation}
\operatorname{Pr}(y=1 | x) \approx P_{A, B}(f) \equiv \frac{1}{1+\exp (A f+B)}, \text { where } f=\mathrm{f}(x)
\end{equation}

let each $f_{i}$ be an estimate of $f(x_{i})$.The best parameter setting $z^{**}=(A^{***},B^{*})$ is determined by solving the following regularized maximum likelihood problem (with $N_{+}$ of the $y_{i}$'s positive,and $N_{-}$ negative):

$$
\begin{aligned} \min_{z=(A, B)} & F(z)=-\sum_{i=1}^{l}\left(t_{i} \log \left(p_{i}\right)+\left(1-t_{i}\right) \log \left(1-p_{i}\right)\right) \\ \text { for } & p_{i}=P_{A, B}\left(f_{i}\right), \text { and } t_{i}=\left\{\begin{array}{ll}{\frac{N_{+}+1}{N_{-}+2}} & {\text { if } y_{i}=+1} \\ {\frac{1}{N_{-}+2}} & {\text { if } y_{i}=-1}\end{array}, i=1, \ldots, l\right.\end{aligned}
$$

 *在 platt 的文章中，他认为 we train an svm, then train the parameters of an additional sigmoid function to map the svm outputs into probabilites, posterior probabilites are also required when a classifier is making a small part of an overall decision.这段话表明，当样本有偏时就必须校准。需要计算后验概率的方式求得对原估计进行校准。*

这篇文章的背景在于作者想对 SVM 的模型的分类输出转成概率输出。

one method of producing probabilistic outputs from a kernel machine was proposed by wahba.wahba used a logistic link function,
\[
P(\text { class } | \text { input })=P(y=1 | \mathbf{x})=p(\mathbf{x})=\frac{1}{1+\exp (-f(\mathbf{x}))}
\]

这里的 $f(x)$ 是原始模型的输出。



*** 样条方法

在真实的生活中，函数 \(f(x)\) 是 $X$ 的线性函数的情况很少见。对于回归问题，通常 $f(X)=E(Y|X)$ 在 $X$ 上是非线性和非可加的。对于线性模型，你可以将其理解为 $f(x)$ 的一阶泰勒展开

在非线性回归模型中，基展开（basis expansion）是运用较多的方法。假设 $h _ { m } ( X ) : \mathbb { R } ^ { p } \mapsto \mathbb { R }$ 为 $X$ 的第 $m$ 个转化变量， $m=1,\cdots,M$.则模型可表述为：

\[
f ( X ) = \sum _ { m = 1 } ^ { M } \beta _ { m } h _ { m } ( X )
\]

其为 $X$ 的线性基展开。不难看出在确定 $h_{m}$ 后，模型对拓展后的输入变量为线性，可以用之前介绍的方法拟合。

一组典型的样条基函数，形式如下：

\begin{aligned} h _ { 1 } ( X ) & = 1 , h _ { 3 } ( X ) = X ^ { 2 } , h _ { 5 } ( X ) = \left( X - \xi _ { 1 } \right) _ { + } ^ { 3 } \\ h _ { 2 } ( X ) & = X , h _ { 4 } ( X ) = X ^ { 3 } , h _ { 6 } ( X ) = \left( X - \xi _ { 2 } \right) _ { + } ^ { 3 } \end{aligned}

其中，截幂函数的形式为：

\begin{aligned} h _ { j } ( X ) & = X ^ { j - 1 } , j = 1 , \ldots , M \\ h _ { M + l } ( X ) & = \left( X - \xi _ { l } \right) _ { + } ^ { M - 1 } , l = 1 , \ldots , K \end{aligned}

为啥选择三次样条?因为三次样条号称是阶数最低的肉眼无法分辨结点连续程度的样条函数。

在样条理论中，需要确定样条的阶数、结点的个数及结点的位置。一个简单的方法是将基函数的个数或自由度作为样条模型的参数，再从观测样本 $x_{i}$ 的范围决定结点的位置。截幂基函数虽然定义简洁，但大数的幂运算可能导致严重的取整问题。B样条基函数可以在结点数量 $K$ 很大时仍然可以快速地计算结果。

如何求解 $f(x)$?

\[
\operatorname { RSS } ( f , \lambda ) = \sum _ { i = 1 } ^ { N } \left\{ y _ { i } - f \left( x _ { i } \right) \right\} ^ { 2 } + \lambda \int \left\{ f ^ { \prime \prime } ( t ) \right\} ^ { 2 } d t
\]

# 统计检验
> 任何检验都是为了否定（而不是为了肯定什么）而设立

## 方差齐次性

为啥要做方差齐次性检验？

关于方差分析的基本假定有三个:

*** 可加性

方差分析的每一次观察值都包含了总体平均数、各因素主效应、各因素间的交互效应、随机误差等许多部分，这些组成部分必须以叠加的方式综合起来，即每一个观察值都可视为这些组成部分的累加和。

*** 正态性
 即随机误差$\epsilon$必须为相互独立的正态随机变量。这也是很重要的条件，如果它不能满足，则均方期望的推导就不能成立，采用 

*** 方差齐性

所谓方差齐性，也就是方差相等，在 t 检验和方差分析中，都需要满足这一前提条件。在两组和多组比较中，方差齐性的意思很容易理解，无非就是比较各组的方差大小，看看各组的方差是不是差不多大小，如果差别太大，就认为是方差不齐，或方差不等。如果差别不大，就认为方差齐性或方差相等。当然，这种所谓的差别大或小，需要统计学的检验，所以就有了方差齐性检验。


然而在线性回归中，理论上\(X\)是有方差的。然而这种理论上的方差，除非你知道总体中每个\(X\)取值上的所有对应\(Y\)的值，否则你是没有办法真正去计算方差的。但这种情况几乎是不可能发生的，因此在线性回归中的方差齐性检验，很多情况下只是一种探测而已。既然线性回归无法做到对每一个\(X\)取值上的\(Y\)值计算方差，那我们可以放宽一下，可以简单地看某一\(X\)取值范围内的\(Y\)值的方差，这是可以做到的。所以实际中我们经常通过线性回归的残差图来判断方差齐性，即以因变量残差作为纵坐标，以某自变量作为横坐标，绘制散点图。如果残差总的来说时随机分布的，没有随着自变量的增加而有其它趋势，基本就可以认为方差齐性。

方差齐性检验方法：
绘制散点图：一般情况因变量是纵轴，但是，在方差齐性检验中，因变量被设置为横轴，纵轴是学生化残差。原因就是，要弄清究竟因变量和残差之间有没有关系。结果说明：如果残差随机分布在一条穿过零点的水平直线的两侧，就说明残差独立，也就是证明因变量方差齐性。

- 离群点检测

残差杠杆图可以告诉我们哪个观测值（如果有）会对模型造成过度影响，换句话说，是否存在我们应该关注的异常值。鉴别强影响点的统计量是 *库克距离* ，一般认为，如果这个统计量的值大于 1，就需要进行更深入的检查。

* 无监督模型
知名的无监督模型包括：一系列的聚类算法，包括 k-平均算法、分层聚类分析、最大期望算法以及可视化和降维算法，包括，主成分分析（PCA）、核主成分分析（kernel PCA）、局部线性嵌入（LLE）、t- 分布随机近邻嵌入（t- SNE）以及关联规则学习，比如，apriori/eclat 等。 
** k-means
** Gussian mixture model

\begin{array}{l}{Y_{i} | X_{i} \text { independent for } i=1, \ldots, n} \\ {Y_{i}\left|X_{i}=x \sim f_{\xi}(y | x) d y \text { for } i=1, \ldots, n\right.} \\ {f_{\xi}(y | x)=\sum_{r=1}^{k} \pi_{r} \frac{1}{\sqrt{2 \pi} \sigma_{r}} \exp \left(-\frac{\left(y-x^{T} \beta_{r}\right)^{2}}{2 \sigma_{r}^{2}}\right)^{2}} \\ {\xi=\left(\beta_{1}, \ldots, \beta_{k}, \sigma_{1}, \ldots, \sigma_{k}, \pi_{1}, \ldots, \pi_{k-1}\right) \in \mathbb{R}^{k p} \times \mathbb{R}_{>0}^{k} \times \Pi} \\ {\Pi=\left\{\pi ; \pi_{r}>0 \text { for } r=1, \ldots, k-1 \text { and } \sum_{r=1}^{k-1} \pi_{r}<1\right\}}\end{array}

这里需要注意分布密度权重等于 1.上述的说明每个成分有着不同的方差和均值。

Thereby, \(X_{i} \in \mathbb{R}^{p}\) are fixed or random covariates, \(Y_{i} \in \mathbb{R}\) is a univariate response variable and
\(\xi=\left(\beta_{1}, \ldots, \beta_{k}, \sigma_{1}, \ldots, \sigma_{k}, \pi_{1}, \ldots, \pi_{k-1}\right)\) denotes the \((p+2) \cdot k-1\) free parameters and \(\pi_{k}\) is given by
\(\pi_{k}=1-\sum_{r=1}^{k-1} \pi_{r} .\) The model in ( 2.1) is a mixture of Gaussian regressions, where every component
\(r\) has its individual vector of regressions coefficients \(\beta_{r}\) and error variances \(\sigma_{r}^{2} .\) We are particularly
interested in the case \(p \gg n\).

模型似然可以写作：
\begin{equation}
 \ell(\theta ; Y)=\sum_{i=1}^{n} \log \left(\sum_{r=1}^{k} \pi_{r} \frac{\rho_{r}}{\sqrt{2 \pi}} \exp \left(-\frac{1}{2}\left(\rho_{r} Y_{i}-X_{i}^{T} \phi_{r}\right)^{2}\right)\right) 
\end{equation}

#+BEGIN_SRC R :exports both :results graphics :file ./fig_1.png
  library(ggplot2)
  library(dplyr)
  p <- ggplot(faithful,aes(x = waiting)
             )+geom_density()
  p
  library(mixtools)
  set.seed(1)
  wait <- faithful$waiting
  mixmdl <- normalmixEM(wait, k = 2)
  mixmdl
#+END_SRC

* 半监督学习
有些算法可以处理部分标记的训练数据——通常是大量未标记数据和少量的标记数据。

* 不平衡数据
**  SMOTE 算法

- 对于少数类 $(X)$ 中每一个样本 $x_{i}$, 计算它到少数类样本集 $(X)$ 中所有样本的距离，得到其 $K$ 个近邻

- 根据样本不平衡比例设置一个采样比例以确定采样倍率 sampling_rate, 对于每一个少数类样本 $x_{i}$, 从其 $k$ 近邻中随机选择 sampling_rate 个近邻为 $x^{1},x^{2},\cdots,x^{sampling_{rate}}$   

- 对于每一个随机选出的近邻 $x^{i},(i=1,2,\cdots,sampling_rate)$ ,分别与原样本按照如下的公式构建新的样本

\[
New=x+rand(0,1) \times (x_{i}-x),i=1,2,\cdots,N
\]

其中， $x_{i}$ 为少数类中的一个观测值， $y_{j}$ 为 $k$ 个邻近中随机抽取的样本。

- 将新样本与原始数据合成，产生新的训练集

* 机器学习
** 感知机

感知机是根据输入实例的特征向量 $x$ 对其进行二分类的线性分类模型：

\[
f(x)=\operatorname{sign}(w \cdot x+b)
\]

感知机模型对应于输入空间（特征空间）中的分离超平面 $w * x +b = 0$.

感知机学习的策略是最小化损失函数：

#+begin_src python
import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
iris = load_iris()
  
#+end_src

#+RESULTS:






** TODO 集成学习

stacking 是一种组合分类器的方法，以两层为例，第一层由多个基学习器组成，其输入为原始训练集，第二层的模型则是以第一层基学习器的输入作为训练集进行再训练（一般用 LR 进行回归组合），从而得到完整的 stacking 模型。


bibliographystyle:natbib	
bibliography:~/Documents/坚果云/我的坚果云/学习/bibliography/ref.bib




