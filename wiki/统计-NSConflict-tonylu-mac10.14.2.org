#+TITLE: 统计理论wiki
# #+SETUPFILE: /Users/luyajun/org-html-themes/setup/theme-bigblow.setup
#+OPTIONS: TeX:t LaTeX:t TOC:nil
#+latex_class: article
#+latex_compiler: xelatex
#+OPTIONS: author:nil email:nil creator:nil timestamp:nil html-postamble:nil TOC:t
# #+HTML_HEAD: <link href="/Users/luyajun/Documents/坚果云/我的坚果云/github/code/css/org-mode.css" rel="stylesheet" type="text/css">
#+STARTUP: inlineimages

 #+BEGIN_SRC quote
如果你对数据绝对没有任何假设，那么你没有理由会更偏好于某个模型。  —— “没有免费午餐定理”
有时，发现问题，比解决问题更困难。
 #+END_SRC
* 问题

* 概率论
** 收敛理论
依概率收敛：
\begin{equation}
 \lim _{n \rightarrow \infty} p\left(\left|x_{n}-x\right| \geqslant \varepsilon\right)=0 \quad x_{n} \stackrel{p}{\rightarrow} x 
\end{equation}
以概率 1 收敛：
\begin{equation}
 P\left(\lim_{n \rightarrow \infty} X_{n}=X\right)=1 \quad x_{n} \stackrel{\text { a.s. }}{\rightarrow} X 
\end{equation}

依分布收敛：
\begin{equation}
 \lim _{n \rightarrow \infty} p\left(x_{n} \leqslant x\right)=P(X \leqslant x) \quad X_{n} \stackrel{d}{\rightarrow} x 
\end{equation}

可以证明依概率收敛可以推出依分布收敛。

* 有监督模型 

知名的有监督模型包括：K-近邻算法、线性回归、逻辑回归、支持向量机、决策树和随机森林、神经网络等。

** 回归模型理论

根据自变量因子的性质，可以将线性模型分为三类：

1、凡自变量因子都是数量因子， **就称为这个模型是回归分析模型**;

2、如果自变量因子均为属性变量， **则称为模型是方差分析模型**;

3、倘若自变量因子中，既有属性因子，也有数量因子, **就称为协方差分析模型**.
** 一般线性回归模型

假定因变量 $Y$ 和自变量 $X$ 满足线性回归模型，其方程为：

\[
Y=X\beta+\epsilon
\]

式中，因变量 $Y$ 为 $n$ 维向量；自变量 $X$ 为 $n\times p$ 矩阵；误差项 $\epsilon$ 为 $n$ 维向量。需要注意的是在简单回归中，误差项 $\epsilon$ 的元素一般要求是独立同分布零均值的，而通常分布假定为正态的，在最小二乘回归的标准输出中，对系数的 $t$ 检验和方差分析的 F 检验，常常认为 p 值小就意味着“显著”，但需要注意误差是否偏离正态性，如果不考虑正态性或者渐近正态性不成立，那么 t 检验和 F 检验就没有任何意义。

在模型比较过程中，需要注意的是对于不满足正态性假定的模型也可以进行互相比较，但所用方法不是这些基于正态性的检验，可以用 AIC 之类的准则或交叉验证来比较。

#+begin_src python
import numpy as np
from sklearn.linear_model import LinearRegression
X=np.array([[1,1],[1,2],[2,2],[2,3]])
y = np.dot(X, np.array([1, 2])) + 3
reg = LinearRegression().fit(X, y)
reg.score(X, y)
reg.coef_
reg.intercept_
reg.predict(np.array([[3, 5]]))
#+end_src


#+begin_src python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
from sklearn.linear_model import LinearRegression
from sklearn.isotonic import IsotonicRegression
from sklearn.utils import check_random_state
n = 100
x = np.arange(n)
rs = check_random_state(0)
y = rs.randint(-50, 50, size=(n, )) + 50.*np.log1p(np.arange(n))

################fit isotonic-regression
ir = IsotonicRegression()
y_ = ir.fit_transform(x, y)
lr = LinearRegression()
lr.fit(x[:, np.newaxis], y)

segments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]
lc = LineCollection(segments, zorder=0)
lc.set_array(np.ones(len(y)))
lc.set_linewidths(np.full(n, 0.5))

fig = plt.figure()
plt.plot(x, y, "r.", markersize=12)




  
#+end_src


#+BEGIN_SRC Python
import numpy as np
import scipy as sp
from scipy.optimize import leastsq
import matplotlib.pyplot as plt
# 目标函数
def real_func(x):
    return np.sin(2*np.pi*x)

# 多项式
def fit_func(p, x):
    f=np.polyy1d(p)
    return f(x)

# 残差
def residuals_func(p, x, y):
    ret=fit_func(p, x) - y
    return ret

# 构造10个点
x=np.linspace(0, 1, 10)
x_points=np.linspace(0, 1, 1000)

#+END_SRC 

#+BEGIN_SRC Python
from sklearn import line
from sklearn import linear_model

#+END_SRC

** TODO 回归诊断
** 方差分析（ANOVA）
*** 单因素方差分析

单因素方差分析是指两个样本平均数比较的引申，它是用来检验多个平均数之间的差异，从而确定因素对试验结果有无显著性影响的一种统计方法。

与通常的统计推断问题一样，方差分析的任务也是先根据实际情况提出原假设 $H_{0}$ 与 备择假设 $H_{1}$, 然后寻找适当的检验统计量进行假设检验。

一般，单因素方差分析的原假设是各组均值或方差相等。卡方值过小的话，会接受原假设即认为各自均值相等。 

** logistic 回归模型
*** 模型形式
Sppose the response variable $Y_{i}$ for $i=1,\cdots,n_{i}$ is binomially distributed $B(n_{i},p_{i})$ so that:
\begin{equation}
P\left(Y_{i}=y_{i}\right)=\left(\begin{array}{l}{n_{i}} \\ {y_{i}}\end{array}\right) p_{i}^{y_{i}}\left(1-p_{i}\right)^{n_{i}-y_{i}}
\end{equation}

we further assume that the $Y_{i}$ are independent.The individual trials that compose the response $Y_{i}$ are all subject to the same $q$ predictors $(x_{i1},\cdot,x_{iq})$.The group of trials is known as a /covariate class/. we need a model that describes the relationship of $x_{1},\cdot,x_{q}$ to $p$.Following the linear model approach, we construct a /linear predictor/:

\begin{equation}
\eta_{i}=\beta_{0}+\beta_{1} x_{i 1}+\ldots+\beta_{q} x_{i q}
\end{equation}

we have already seen above that setting $\eta_{i}=p_{i}$ is not appropriate because we require $0 \leq p_{i} \leq 1$.Instead we shall use a link function $g$ such that $\eta_{i}=g(p_{i})$.For this application,we shall need $g$ to be monotone and be such that $0\leq g^{-1}(\eta)\leq 1$ for any $\eta$.There are three common choices: 

1.logit:$\eta=log(p/(1-p))$.
2.probit:$\eta=\Phi^{-1}(p)$ where $\Phi^{-1}$ is the inverse normal cumulative distribution function.
3.Complementary log-log:$\eta=\log(-log(1-p))$


\[
\operatorname{logit}\left(p_{i}\right)=\log \left(\frac{p_{i}}{1-p_{i}}\right)=\beta_{0}+\beta_{1} x_{1 i}+\cdots+\beta_{k} x_{k i}
\]

$$
p_{i}=\frac{\exp \left(\beta_{0}+\beta_{1} x_{1 i}+\cdots+\beta_{k} x_{k i}\right)}{1+\exp \left(\beta_{0}+\beta_{1} x_{1 i}+\cdots+\beta_{k} x_{k i}\right)}
$$

观测 $y_{i}$ 服从于一个二项分布，均值是 $n_{i}p_{i}$,能够表示为 $y_{i}=n_{i}p_{i}+\epsilon_{i}$,残差部分 $\epsilon_{i}=y_{i}-n_{i}p_{i}$ 是零均值，但是不再服从的是二项分布，实际上， $\epsilon$ **服从的是位移二项分布**.

需要补充的是：$E\left(\varepsilon_{i} | X_{i}\right)=0$,即给定 X 的前提下，$\varepsilon_{i}$ 的期望为 0. 

\begin{equation}
 \varepsilon_{j}=\left\{\begin{array}{ll}{1-X_{j}^{\prime} \beta} & {\left(Y_{i}=1\right)} \\ {-X_{i}^{\prime} \beta} & {\left(Y_{j}=0\right)}\end{array}\right. 
\end{equation}

上式为 logistic 回归模型的残差，可以看出是二元变量，而不是我们通常假定的正态分布。

【问题？】
什么是位移二项分布？

似然函数

$$
L(\boldsymbol{\beta})=\prod_{i=1}^{n}\left(\begin{array}{l}{n_{i}} \\ {y_{i}}\end{array}\right) p_{i}^{y_{i}}\left(1-p_{i}\right)^{n_{i}-y_{i}}
$$

\begin{aligned} \log L(\boldsymbol{\beta}) &=\sum_{i=1}^{n}\left\{\log \left(\begin{array}{l}{n_{i}} \\ {y_{i}}\end{array}\right)+y_{i} \log p_{i}+\left(n_{i}-y_{i}\right) \log \left(1-p_{i}\right)\right\} \\ &=\sum_{i=1}^{n}\left\{\log \left(\begin{array}{c}{n_{i}} \\ {y_{i}}\end{array}\right)+y_{i} \log \left(\frac{p_{i}}{1-p_{i}}\right)+n_{i} \log \left(1-p_{i}\right)\right\} \\ &=\sum_{i=1}^{n}\left\{\log \left(\begin{array}{l}{n_{i}} \\ {y_{i}}\end{array}\right)+y_{i} \eta_{i}-n_{i} \log \left(1+e^{\eta_{i}}\right)\right\} \end{aligned}


$$
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_{j}}=\sum_{i=1}^{n} y_{i} x_{j i}-\sum_{i=1}^{n} n_{i} x_{j i} e^{\eta_{i}}\left(1+e^{\eta_{i}}\right)^{-1}, \quad j=0,1, \ldots, k
$$

可知，上式是没有办法求解 $\beta$ 的精确解，只能求得数值解。一个广泛的求解方法就是 Fisher ’s 得分法，此法相当于是一个重复加权最小二乘方法, $z_{i}=\eta_{i}+(y_{i}-n_{i}p_{i})/\{n_{i}p_{i}(1-p_{i})\}$,weight 等于 $n_{i}p_{i}(1-p_{i})$.

一些有用的结论：

- 随着样本量 $n$ 的增加，二项分布近似于正态分布.随机变量 $Z=\frac{Y-n p}{\sqrt{\{} n p(1-p)\}}$ 接近正态分布。McCullagh 等人证明当 $np(1-p)\ge2$,随机变量 Y 即可满足正态分布假设，特别是当 $p$ 接近于 0.5 的时候，所以当 n 等于 10 的时候，二项分布近似于正态分布。

在样本量足够大时，二项分布近似于正态分布。其实也可以利用线性模型拟合逾期率：

$y_{i}=0$ 为失败，$y_{i}=1$ 为成功。$E(Y_{i})=n_{i}p_{i}$,$var(Y_{i})=n_{i}p_{i}(1-p_{i})$

$$
\sum_{i=1}^{n}\left(\frac{y_{i}}{n_{i}}-p_{i}\right)^{2}=\sum_{i=1}^{n}\left(\tilde{p}_{i}-\beta_{0}-\beta_{1} x_{1 i}-\cdots-\beta_{k} x_{k i}\right)^{2}
$$

但是这种方法有很多的缺点：比如，异方差问题，$\operatorname{var}\left(\tilde{p}_{i}\right)=p_{i}(1-p_{i})/n_{i}$,当 $p_{i}$ 在 0.25-0.75 时, \(0.19 < p_{i}(1-p_{i}) < 0.25\) 也就是方差不会相差很大，但是 p 值很大或者很小的时候，那么方差变化就会很大！一种解决的方法就是加权回归模型 $\sum_{i=1}^{n} w_{i}\left(\tilde{p}_{i}-p_{i}\right)^{2}$.第二个问题就是正态分布，当 n 很大的时候，这个问题不存在。第三个问题就是，估计值可能是负数！而 $\hat{p}$ 不可能是负数！

所以，需要对成功概率 $p$ 作 logit 变换 $log(p/(1-p))$,可以写作 $logit(p)$.logit 变换后，值域就变成了 $(-\inf,\inf)$.

simulation of glm

#+BEGIN_SRC R :exports both :results graphics :file ./fig_1.png
  ##两个特征高度相关
  library(MASS)
  n=1000
  #inv.logit 其实就是 P
  inv.logit <- function(p){
      return(exp(p)/(1+exp(p)))
  }
  Sigma <- matrix(c(1,0.9,0.9,1),2,2)
  X=mvrnorm(n = 1000, rep(0, 2), Sigma)
  beta1=c(0.5,1.5)
  Y=rbinom(n,1,inv.logit(1+X%*%beta1+rnorm(1000,0,1)))
  data=data.frame(Y,X)
  glm(Y~1+.,data = data,family = "binomial")
  #####特征重复2份
  library(MASS)
  x1=rnorm(1000,mean = 0,sd=1)
  X=matrix(rep(x1,2),nrow = 1000)
  beta1=c(0.5,1.5)
  Y=rbinom(n,1,inv.logit(1+X%*%beta1+rnorm(1000,0,1)))
  data=data.frame(Y,X)
  glm(Y~1+.,data = data,family = "binomial")
#+END_SRC

*** 模型推断

假设有

*** 变量编码
**** woe 编码


#+BEGIN_SRC R :exports both :results graphics :file ./fig_1.png
mifi_model_feature_woe_encoding_all(df, feat_cuts, category_feature_names = NULL,
  label_identify, encoding_path, missing_val = -1, is_debug = F)
#+END_SRC

**** one-hot 编码

**** dummy 编码

*** 模型评价
**** 混淆矩阵

|      | 预测                   |                  |
| 实际 | 1                      | 0                |
|    1 | d,true positive        | c,false negative |
|    0 | b,false positive       | a,true negative  |
|      | b+d,predicted positive | a+c,predicted negative       |

从上表可以看出，positive 和 negative 的概念均是相对于预测集而言。

几组常用的评估指标：

1. 准确（分类）率 vs 误分率

准确（分类）率 = 正确预测的正反例数 / 总数

Accuracy=true positive and true negative/total cases= a+d/a+b+c+d.

误分类率 = 错误预测的正反例数 / 总数

Error rate=false positive and false negative/total cases=b+c/a+b+c+d=1-Accuracy

2.（正例的）覆盖率 VS.（正例的）命中率

覆盖率 = 正确预测到的正例数 / 实际正例总数

Recall(True Positive Rate，or Sensitivity)=true positive/total actual positive=d/c+d

#+begin_quote
注：覆盖率 (Recall）这个词比较直观，在数据挖掘领域常用。因为感兴趣的是正例 (positive)，比如在信用卡欺诈建模中，我们感兴趣的是有高欺诈倾向的客户，那么我们最高兴看到的就是，用模型正确预测出来的欺诈客户 (True Positive)cover 到了大多数的实际上的欺诈客户，覆盖率，自然就是一个非常重要的指标。这个覆盖率又称 Sensitivity， 这是生物统计学里的标准词汇，SAS 系统也接受了（谁有直观解释？）。 以后提到这个概念，就表示为, Sensitivity（覆盖率，True Positive Rate）
#+end_quote

命中率 = 正确预测到的正例数/预测正例总数

Precision(Positive Predicted Value,PV+)=true positive/ total predicted positive=d/b+d

#+begin_quote
注：这是一个跟覆盖率相对应的指标。对所有的客户，你的模型预测，有 b+d 个正例，其实只有其中的 d 个才击中了目标（命中率）。在数据库营销里，你预测到 b+d 个客户是正例，就给他们邮寄传单发邮件，但只有其中 d 个会给你反馈（这 d 个客户才是真正会响应的正例），这样，命中率就是一个非常有价值的指标。 以后提到这个概念，就表示为 PV+(命中率，Positive Predicted Value)。
#+end_quote

3.（负例的）覆盖率 VS.（负例的）命中率

负例的覆盖率 = 正确预测到的负例个数 / 实际负例总数

Specificity(True Negative Rate)=true negative/total actual negative=a/a+b

负例的命中率 = 正确预测到的负例个数 / 预测负例总数

Negative predicted value(PV-)=true negative/total predicted negative=a/a+c

*** 广义线性混合模型（glmm）
成组的数据通常存在在任何的领域，特别对于纵向或者空间数据。对于分析成组的数据的时候，由于组内数据具有更为相似的相关性，所以再假定样本间的独立性便不再合适，所以需要引入随机效应的概念。

给定一个随机效应 $\alpha$,观测值 $y_{1},\cdots,y_{n}$ 是条件独立的，以至于 $y_{i} \sim N(x^{'}_{i}\beta+z^{'}_{i}\alpha,\tau^2)$,$x_{i},z_{i}$ 是已知的向量，$\tau^2$ 是未知的方差参数。

glmm 中，两个重要的特性是条件独立性，给定随机效应时，服从一个条件分布；第二个是随机效应的分布。
#+BEGIN_QUOTE
In mixed-effects models at least one of the covariates is a categorical covariate representing experimental or observational “units” in the data set.cite:Bates
#+END_QUOTE

这说明在混合模型中，必须要有一个分类变量，如果特征是一个随机样本，那么这个模型就是随机效应模型。固定效应和随机效应的区别在于是否是分类变量。随机效应不是一个参数而是一个未观测随机变量。

**** Definitions 

A mixed model incorporates two random variables:\(\mathscr{B}\),the $q$ -dimensional vector of random effects, and \(\mathscr{Y}\),the $n$ -dimensional response vector.In a linear mixed model the unconditional distribution of $\mathscr{B}$ and the conditions distribution,\((\mathscr{Y} | \mathscr{B}=\mathbf{b})\), are both mulvariate Gaussian (or "normal") distribution,

\begin{aligned}
(\mathscr{Y} | \mathscr{B}=&\mathbf{b}) \sim \mathscr{N}\left(\mathbf{X} \beta+\mathbf{Z} \mathbf{b}, \sigma^{2} \mathbf{I}\right) \\ \mathscr{B} & \sim \mathscr{N}\left(\mathbf{0}, \Sigma_{\theta}\right) 
\end{aligned}

The conditional mean of $\mathscr{Y}$,given $\mathscr{B}=\mathbf{b}$, is the linear predictor, $\mathbf{X} \beta+\mathbf{Z} \mathbf{b}$,which depends on the $p$-dimensional fixed-effects parameter, $\beta$ ,and on $\mathbf{b}$.The model matrices, $\mathbf{X}$ and $\mathbf{Z}$,of dimension $n\times p$ and $n\times q$,respectively,are determined from the formula for the model and the values of covariates. *Although the matrix $\mathbf{Z}$ can be large (i.e. both $n$ and $q$ can be large), it is sparse (i.e. most of the elements in the matrix are zero).*

** TODO 待写

the relative covariance factor, $

#+BEGIN_SRC R :results output graphics :file fig_2.png :exports both
    library(lme4)
    library(ggplot2)
    library(tidyverse)
    str(Dyestuff)
    head(Dyestuff)
    summary(Dyestuff)
    ## Batch     Yield     
    ## A:5   Min.   :1440  
    ## B:5   1st Qu.:1469  
    ## C:5   Median :1530  
    ## D:5   Mean   :1528  
    ## E:5   3rd Qu.:1575  
    ## F:5   Max.   :1635
    ###从Batch 的数据结构可以看出是平衡的
    p <- ggplot(Dyestuff, aes(x=Batch, y=Yield)) + geom_boxplot()
    p #可以看出不同组数据的均值不同
  #+END_SRC

  #+RESULTS:
  [[file:fig_2.png]]

  An example of linear mixed model in r 

  #+BEGIN_SRC R :exports both :results graphics :file ./fig_1.png
  library(lme4)
  fm01 <- lmer(Yield~ 1|Batch,Dyestuff,REML = FALSE,verbose = 1)
  summary(fm01)
  fm01@pp
  #+END_SRC

*** 模型校准
- 缘由
一个分类模型能否用于预测垃圾邮件、一个分子的毒性状态， *我们期望估计的类概率更能代表样本真实潜在的概率。* (在小米小贷的应用场景中，由于样本自身的偏倚性，意思就是没有拒绝样本的表现，所以样本真实的拒绝概率势必需要校准。)为了达到良好校准，概率必须有效反映出感兴趣事件的真实似然。以垃圾邮件过滤为例，如果模型预测一封邮件为垃圾邮件的概率（或概率类似值）为 20%，若平均每 5 个样本中真正有 1 个类似类型的邮件， *那么这个概率是良好校准的* 。

 *一种评估类概率的方法* 是 *校准图* （calibration plot），对于给定的数据集，校准图展示 *事件的观测概率与预测的类概率之间的对比度量* 。创建校准图的一种方法是用 分类模型对收集的已知结果（测试集效果更佳）样本进行打分，接下来基于样本的类概率对数据封箱成不同的组。例如，封箱集合可能是[0,10%],(10%,20%],...,(90%,100%].

对于每一箱，确定观测事件发生率，假定有 50 个样本落入[0,10%]箱子且仅有一个事件发生，则箱的中点是 5%，观测事件的发生率为 2%。校准图以箱的中点为 x 轴，观测事件发生率为 y 轴绘图。如果点落在沿 45%的对角线上，模型就具备良好的校准率。

 *总结：这个校准图方法可以作为不同模型估计效果的比较来看。*

如果发现模型低估了事件的概率，那么怎么办？我们可以创建另外一个模型对该模式进行调整。我们可以通过以下公式，基于训练集的类预测和真实结果对估计的概率进行后续处理：

\begin{equation}
\hat{p}^{*}=\frac{1}{1+\exp \left(-\beta_{0}-\beta_{1} \hat{p}\right)}
\end{equation}

其中，通过非校准类概率（$\hat{p}$）的函数预测真实类来估计参数。

instead of predicting the label, many applications require a posterior class probability \(P(y=1|x)\).Platt(2000) proposes approximating the posterior by a sigmoid Function
\begin{equation}
\operatorname{Pr}(y=1 | x) \approx P_{A, B}(f) \equiv \frac{1}{1+\exp (A f+B)}, \text { where } f=\mathrm{f}(x)
\end{equation}

let each $f_{i}$ be an estimate of $f(x_{i})$.The best parameter setting $z^{**}=(A^{***},B^{*})$ is determined by solving the following regularized maximum likelihood problem (with $N_{+}$ of the $y_{i}$'s positive,and $N_{-}$ negative):

$$
\begin{aligned} \min_{z=(A, B)} & F(z)=-\sum_{i=1}^{l}\left(t_{i} \log \left(p_{i}\right)+\left(1-t_{i}\right) \log \left(1-p_{i}\right)\right) \\ \text { for } & p_{i}=P_{A, B}\left(f_{i}\right), \text { and } t_{i}=\left\{\begin{array}{ll}{\frac{N_{+}+1}{N_{-}+2}} & {\text { if } y_{i}=+1} \\ {\frac{1}{N_{-}+2}} & {\text { if } y_{i}=-1}\end{array}, i=1, \ldots, l\right.\end{aligned}
$$

 *在 platt 的文章中，他认为 we train an svm, then train the parameters of an additional sigmoid function to map the svm outputs into probabilites, posterior probabilites are also required when a classifier is making a small part of an overall decision.这段话表明，当样本有偏时就必须校准。需要计算后验概率的方式求得对原估计进行校准。*

这篇文章的背景在于作者想对 SVM 的模型的分类输出转成概率输出。

one method of producing probabilistic outputs from a kernel machine was proposed by wahba.wahba used a logistic link function,
\[
P(\text { class } | \text { input })=P(y=1 | \mathbf{x})=p(\mathbf{x})=\frac{1}{1+\exp (-f(\mathbf{x}))}
\]

这里的 $f(x)$ 是原始模型的输出。



*** 样条方法

在真实的生活中，函数 \(f(x)\) 是 $X$ 的线性函数的情况很少见。对于回归问题，通常 $f(X)=E(Y|X)$ 在 $X$ 上是非线性和非可加的。对于线性模型，你可以将其理解为 $f(x)$ 的一阶泰勒展开

在非线性回归模型中，基展开（basis expansion）是运用较多的方法。假设 $h _ { m } ( X ) : \mathbb { R } ^ { p } \mapsto \mathbb { R }$ 为 $X$ 的第 $m$ 个转化变量， $m=1,\cdots,M$.则模型可表述为：

\[
f ( X ) = \sum _ { m = 1 } ^ { M } \beta _ { m } h _ { m } ( X )
\]

其为 $X$ 的线性基展开。不难看出在确定 $h_{m}$ 后，模型对拓展后的输入变量为线性，可以用之前介绍的方法拟合。

一组典型的样条基函数，形式如下：

\begin{aligned} h _ { 1 } ( X ) & = 1 , h _ { 3 } ( X ) = X ^ { 2 } , h _ { 5 } ( X ) = \left( X - \xi _ { 1 } \right) _ { + } ^ { 3 } \\ h _ { 2 } ( X ) & = X , h _ { 4 } ( X ) = X ^ { 3 } , h _ { 6 } ( X ) = \left( X - \xi _ { 2 } \right) _ { + } ^ { 3 } \end{aligned}

其中，截幂函数的形式为：

\begin{aligned} h _ { j } ( X ) & = X ^ { j - 1 } , j = 1 , \ldots , M \\ h _ { M + l } ( X ) & = \left( X - \xi _ { l } \right) _ { + } ^ { M - 1 } , l = 1 , \ldots , K \end{aligned}

为啥选择三次样条?因为三次样条号称是阶数最低的肉眼无法分辨结点连续程度的样条函数。

在样条理论中，需要确定样条的阶数、结点的个数及结点的位置。一个简单的方法是将基函数的个数或自由度作为样条模型的参数，再从观测样本 $x_{i}$ 的范围决定结点的位置。截幂基函数虽然定义简洁，但大数的幂运算可能导致严重的取整问题。B样条基函数可以在结点数量 $K$ 很大时仍然可以快速地计算结果。

如何求解 $f(x)$?

\[
\operatorname { RSS } ( f , \lambda ) = \sum _ { i = 1 } ^ { N } \left\{ y _ { i } - f \left( x _ { i } \right) \right\} ^ { 2 } + \lambda \int \left\{ f ^ { \prime \prime } ( t ) \right\} ^ { 2 } d t
\]

# 统计检验
> 任何检验都是为了否定（而不是为了肯定什么）而设立

## 方差齐次性

为啥要做方差齐次性检验？

关于方差分析的基本假定有三个:

*** 可加性

方差分析的每一次观察值都包含了总体平均数、各因素主效应、各因素间的交互效应、随机误差等许多部分，这些组成部分必须以叠加的方式综合起来，即每一个观察值都可视为这些组成部分的累加和。

*** 正态性
 即随机误差$\epsilon$必须为相互独立的正态随机变量。这也是很重要的条件，如果它不能满足，则均方期望的推导就不能成立，采用 

*** 方差齐性

所谓方差齐性，也就是方差相等，在 t 检验和方差分析中，都需要满足这一前提条件。在两组和多组比较中，方差齐性的意思很容易理解，无非就是比较各组的方差大小，看看各组的方差是不是差不多大小，如果差别太大，就认为是方差不齐，或方差不等。如果差别不大，就认为方差齐性或方差相等。当然，这种所谓的差别大或小，需要统计学的检验，所以就有了方差齐性检验。


然而在线性回归中，理论上\(X\)是有方差的。然而这种理论上的方差，除非你知道总体中每个\(X\)取值上的所有对应\(Y\)的值，否则你是没有办法真正去计算方差的。但这种情况几乎是不可能发生的，因此在线性回归中的方差齐性检验，很多情况下只是一种探测而已。既然线性回归无法做到对每一个\(X\)取值上的\(Y\)值计算方差，那我们可以放宽一下，可以简单地看某一\(X\)取值范围内的\(Y\)值的方差，这是可以做到的。所以实际中我们经常通过线性回归的残差图来判断方差齐性，即以因变量残差作为纵坐标，以某自变量作为横坐标，绘制散点图。如果残差总的来说时随机分布的，没有随着自变量的增加而有其它趋势，基本就可以认为方差齐性。

方差齐性检验方法：
绘制散点图：一般情况因变量是纵轴，但是，在方差齐性检验中，因变量被设置为横轴，纵轴是学生化残差。原因就是，要弄清究竟因变量和残差之间有没有关系。结果说明：如果残差随机分布在一条穿过零点的水平直线的两侧，就说明残差独立，也就是证明因变量方差齐性。

- 离群点检测

残差杠杆图可以告诉我们哪个观测值（如果有）会对模型造成过度影响，换句话说，是否存在我们应该关注的异常值。鉴别强影响点的统计量是 *库克距离* ，一般认为，如果这个统计量的值大于 1，就需要进行更深入的检查。

* 无监督模型
知名的无监督模型包括：一系列的聚类算法，包括 k-平均算法、分层聚类分析、最大期望算法以及可视化和降维算法，包括，主成分分析（PCA）、核主成分分析（kernel PCA）、局部线性嵌入（LLE）、t- 分布随机近邻嵌入（t- SNE）以及关联规则学习，比如，apriori/eclat 等。 
** k-means
** Gussian mixture model

\begin{array}{l}{Y_{i} | X_{i} \text { independent for } i=1, \ldots, n} \\ {Y_{i}\left|X_{i}=x \sim f_{\xi}(y | x) d y \text { for } i=1, \ldots, n\right.} \\ {f_{\xi}(y | x)=\sum_{r=1}^{k} \pi_{r} \frac{1}{\sqrt{2 \pi} \sigma_{r}} \exp \left(-\frac{\left(y-x^{T} \beta_{r}\right)^{2}}{2 \sigma_{r}^{2}}\right)^{2}} \\ {\xi=\left(\beta_{1}, \ldots, \beta_{k}, \sigma_{1}, \ldots, \sigma_{k}, \pi_{1}, \ldots, \pi_{k-1}\right) \in \mathbb{R}^{k p} \times \mathbb{R}_{>0}^{k} \times \Pi} \\ {\Pi=\left\{\pi ; \pi_{r}>0 \text { for } r=1, \ldots, k-1 \text { and } \sum_{r=1}^{k-1} \pi_{r}<1\right\}}\end{array}

这里需要注意分布密度权重等于 1.上述的说明每个成分有着不同的方差和均值。

Thereby, \(X_{i} \in \mathbb{R}^{p}\) are fixed or random covariates, \(Y_{i} \in \mathbb{R}\) is a univariate response variable and
\(\xi=\left(\beta_{1}, \ldots, \beta_{k}, \sigma_{1}, \ldots, \sigma_{k}, \pi_{1}, \ldots, \pi_{k-1}\right)\) denotes the \((p+2) \cdot k-1\) free parameters and \(\pi_{k}\) is given by
\(\pi_{k}=1-\sum_{r=1}^{k-1} \pi_{r} .\) The model in ( 2.1) is a mixture of Gaussian regressions, where every component
\(r\) has its individual vector of regressions coefficients \(\beta_{r}\) and error variances \(\sigma_{r}^{2} .\) We are particularly
interested in the case \(p \gg n\).

模型似然可以写作：
\begin{equation}
 \ell(\theta ; Y)=\sum_{i=1}^{n} \log \left(\sum_{r=1}^{k} \pi_{r} \frac{\rho_{r}}{\sqrt{2 \pi}} \exp \left(-\frac{1}{2}\left(\rho_{r} Y_{i}-X_{i}^{T} \phi_{r}\right)^{2}\right)\right) 
\end{equation}

#+BEGIN_SRC R :exports both :results graphics :file ./fig_1.png
  library(ggplot2)
  library(dplyr)
  p <- ggplot(faithful,aes(x = waiting)
             )+geom_density()
  p
  library(mixtools)
  set.seed(1)
  wait <- faithful$waiting
  mixmdl <- normalmixEM(wait, k = 2)
  mixmdl
#+END_SRC

* 半监督学习
有些算法可以处理部分标记的训练数据——通常是大量未标记数据和少量的标记数据。

* 不平衡数据
**  SMOTE 算法

- 对于少数类 $(X)$ 中每一个样本 $x_{i}$, 计算它到少数类样本集 $(X)$ 中所有样本的距离，得到其 $K$ 个近邻

- 根据样本不平衡比例设置一个采样比例以确定采样倍率 sampling_rate, 对于每一个少数类样本 $x_{i}$, 从其 $k$ 近邻中随机选择 sampling_rate 个近邻为 $x^{1},x^{2},\cdots,x^{sampling_{rate}}$   

- 对于每一个随机选出的近邻 $x^{i},(i=1,2,\cdots,sampling_rate)$ ,分别与原样本按照如下的公式构建新的样本

\[
New=x+rand(0,1) \times (x_{i}-x),i=1,2,\cdots,N
\]

其中， $x_{i}$ 为少数类中的一个观测值， $y_{j}$ 为 $k$ 个邻近中随机抽取的样本。

- 将新样本与原始数据合成，产生新的训练集

* 机器学习
** 感知机

感知机是根据输入实例的特征向量 $x$ 对其进行二分类的线性分类模型：

\[
f(x)=\operatorname{sign}(w \cdot x+b)
\]

感知机模型对应于输入空间（特征空间）中的分离超平面 $w * x +b = 0$.

感知机学习的策略是最小化损失函数：

#+begin_src python
import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
iris = load_iris()
  
#+end_src

#+RESULTS:






** TODO 集成学习

stacking 是一种组合分类器的方法，以两层为例，第一层由多个基学习器组成，其输入为原始训练集，第二层的模型则是以第一层基学习器的输入作为训练集进行再训练（一般用 LR 进行回归组合），从而得到完整的 stacking 模型。


bibliographystyle:natbib	
bibliography:~/Documents/坚果云/我的坚果云/学习/bibliography/ref.bib




